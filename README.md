# Language processing

## Description
Modeling Order in Neural Word Embeddings at Scale is an interesting work on word embeddings, which considers word orders from the well-known word2vec (a.k.a. CBOW and skip-gram, Mikolov et al., 2013). This project will implement the novel paper, which revise the word2vec into a partitioned embedding model. This new approach can include the order information into the word embeddings.

## Plan

### Todo List
- [x] Study the main algorithm and related work.
- [ ] Implement the algorithm.
- [ ] Validate the code and improve the performance.

### Time Schedule


| Stage | Start  | End | Goals |
| ------------- | ------------- | ------------- | ------------- |
| s | 06/07/16  | 06/12/16  | Team match. Read paper and discuss with primary leader. |
| 1 | 06/13/16  | 06/20/16  | Knowledge learning.  |
| 2 | 06/21/16  | 06/27/16  | Implement the paper. |
| 3 | 06/28/16  | 07/04/16  | Validate the code. |
| 4 | 07/05/16  | 07/11/16  | Improve the performance. | 
| e | 07/12/16  | 07/18/16  | Project Report Writing, Demo Video Making |

## Resource
- [BitTiger Project: Language processing](http://www.bittiger.io/microproject/3WHtbKbWFfNgk26Mx)
- [Original paper](http://jmlr.org/proceedings/papers/v37/trask15.pdf)
- [Word2vec package](https://radimrehurek.com/gensim/models/word2vec.html)
- [Word2vec tutorial](http://deeplearning4j.org/word2vec)

## License
See the [LICENSE](LICENSE.md) file for license rights and limitations (MIT).

## Project Information
- category: NLP
- team: G-18
- description: Modeling Order in Neural Word Embeddings at Scale.
- stack: ML, NLP
